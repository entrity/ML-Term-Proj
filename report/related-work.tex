\section{Related Work}

Clustering is a group of methods for unsupervised analysis of data which entails finding data points which lie close together in some space and inferring that neighbours or clusters share properties. Common methods include $k$-means 
k-means \cite{kmeansmacqueen1967some}, Gaussian mixture models, and their many variants.

The salient drawback of applying one of these simple clustering methods is that acting on the original representation of the data may be inefficient or otherwise fail to discover meaningful spatial relationships between points. Spectral clustering \cite{sc1von2007tutorial}, \cite{sc2bach2004learning}, \cite{sc3ng2002spectral} is a family of methods which addresses this shortcoming by projecting the data into an eigenspace of the Laplacian of the similarity matrix.

Other works learn alternative embeddings for the data and operate on the similarity between points in the embedding space. SEC extends spectral clustering by learning an embedding (which is a perturbation of the Laplacian) and operating in an eigenspace of the embedding \cite{secnie2011spectral}.

The foregoing clustering approaches use a fixed representation of the data, but several methods exist to learn a representation to fit the distribution.  \cite{nie2014clustering}, for example, seeks to learn the similarity matrix at the same time that the clusters are inferred. \cite{zeghidour2016deep}, \cite{xie2016unsupervised}, \cite{chang2017deep}, \cite{dscl_law2017deep} use deep learning to provide an embedding.

% Common clustering approaches use a fixed embedding; we can improve by learning an embedding. DSCL has low-cost loss function, which doesn't require running clustering after each batch. Its gradient has a closed-form, linear in size of batch and quadratic in size of embedding.

The crux of Deep Embedding Clustering (DEC) \cite{xie2016unsupervised}'s learned embedding is that unsupervised learning cannot benefit from labels, so the loss function which is used to train the embedding-producing neural network is the KL divergence between two distributions: the network's current distribution of outputs and a tweak of this same distribution in which the weight of data points whose confidence is high (i.e. which are near to a centroid) is increased and in which disproportionately large clusters lose potency.

One approach which surpassed DEC is Deep Spectral Clustering Learning (DSCL) \cite{dscl_law2017deep}. Like DEC, DSCL uses a learned embedding, but it improves on performance and computational cost by formulating a loss with a closed-form gradient. A neural network provides an embedding, and the computational cost of the loss is linear in the size of the batch and quadratic in the size of the embedding. The primary drawback of DSCL is that its loss function make use of the ground truth partition (i.e. clustering) matrix, which is unavailable in an unsupervised setting.

An MLP, such as DEC \cite{xie2016unsupervised} and DSCL \cite{dscl_law2017deep} used, is ill-suited for complex image data, and that mismatch leaves room for the possibility that replacing DEC's SAE with a convolutional autoencoder like that presented by \cite{masci2011stacked} might achieve improved clustering results. A convolutional autoencoder learns an embedding in an unsupervised manner, but it has the benefits of a CNN which are critical for application on image data, primarily spatial flexibility on input data.

Deep Adaptive Clustering (DAC) \cite{chang2017deep} was presented specifically for clustering image data. It uses pairwise judgments, which address the classification problem in a straightforward manner: if the cosine distance between a given pair of images' feature embeddings is approximately 1, they belong in the same cluster; if it is approximately 0, they belong in different clusters. It uses CNN's to deal with image data and seeks to drive the feature embedding toward one-hot vectors. The critical distinction between this approach and DEC is the loss function: DAC uses a pairwise loss function, and DEC uses a loss function which requires an entire mini-batch; DAC's loss pushes the feature vectors toward one-hot encodings, and DEC makes no constraints on the nature of the embeddings.
